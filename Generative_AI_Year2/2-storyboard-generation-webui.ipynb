{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Challenge: Image Generation with WebUI\n",
    "\n",
    "This notebook demonstrates the process of generating images and creating a simple storyboard using the [WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) framework. It provides a basic workflow for converting textual scene descriptions into visual illustrations using a text-to-image model. The focus of this notebook is to familiarize you with WebUI's capabilities for image generation, without implementing any methods to maintain character consistency across different scenes.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Load scene descriptions and convert them into images using WebUI.\n",
    "- Generate visual representations of consecutive scenes for a storyboard.\n",
    "- Understand the limitations of existing models in maintaining consistent character appearances across multiple generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting Up a Python Environment for the Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you need to set up a Python environment for this challenge and install the necessary libraries. Follow these steps:\n",
    "\n",
    "1. **Creating a Python Environment (skip this step if you already created the environment following the instructions of the previous notebook)**: \n",
    "\n",
    "    First, you will create a dedicated Python environment for the challenge. This will allow you to install the required packages without affecting other projects. To do this:\n",
    "    - Open a terminal (or the Anaconda Prompt on Windows or macOS).\n",
    "    - Run the following command to create a new environment. You can name it \"genai_challenge\" or choose your own name:\n",
    "\n",
    "        ``conda create --name genai_challenge``\n",
    "\n",
    "    - After creating the environment, activate it by running the command:\n",
    "        \n",
    "        ``conda activate genai_challenge``\n",
    "        \n",
    "2. **Installing Required Libraries**: \n",
    "\n",
    "    Most libraries needed for this challenge are standard Python libraries and come pre-installed. However, you will need to install the ``requests`` library manually to access the WebUI's API. To install it:\n",
    "    - In the terminal with your activated environment, run the following command:\n",
    "\n",
    "        ``conda install requests``\n",
    "\n",
    "    - If you are not using a Conda environment, but another virtual environment tool, you can use the following command:\n",
    "        \n",
    "        ``pip install requests``\n",
    "\n",
    "3. **Selecting the Environment and Verifying the Installation**\n",
    "\n",
    "    After setting up the environment, you need to ensure that Visual Studio Code uses the correct Python environment for this notebook:\n",
    "\n",
    "    - At the top right corner of the window, click on \"Select Kernel\".\n",
    "    - Select your newly created environment (\"genai_challenge\") from the list of available environments.\n",
    "\n",
    "    To verify that the ``websocket-client`` library is installed correctly, you can run the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "print(\"Requests library installed correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above runs without any errors, you are all set and ready to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Image Generation Using WebUI\n",
    "\n",
    "Let's start by importing the libraries that we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in this example are generated by a Stable Diffusion model running on the ADS&AI server. There are four instances of the WebUI framework available on the server:\n",
    "- buas5.edirlei.com\n",
    "- buas6.edirlei.com\n",
    "- buas7.edirlei.com\n",
    "- buas8.edirlei.com\n",
    "\n",
    "Since all participants of the Generative AI Challenge will be sharing these instances, they may become overloaded at times. If you notice that image generation is taking longer than expected or encounter access errors while trying to run this notebook, try switching to another instance.\n",
    "\n",
    "Next, we'll specify which WebUI instance to use by assigning its address to the variable `webui_server_address`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webui_server_address = \"buas5.edirlei.com\"\n",
    "webui_username = \"adsai\"\n",
    "webui_password = \"uuUP4whjX29cF3cxwrX3SY5Mm9TmtASkdgpaNCTHZ9\"\n",
    "webui_credentials = base64.b64encode(f\"{webui_username}:{webui_password}\".encode('utf-8')).decode('utf-8')\n",
    "webui_auth_header = f\"Basic {webui_credentials}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate images in WebUI from Python, we need to interact with an API endpoint provided by WebUI. The function defined below will handle this process for you, so you don't need to worry about the technical details of accessing this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webui_generate_image(model_name, prompt, negative_prompt, seed, width, height, output_filename):\n",
    "    randseed = (seed != -1)\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"steps\": 6,\n",
    "        \"cfg_scale\": 2,\n",
    "        \"randomize_seed\": randseed,\n",
    "        \"seed\": seed,\n",
    "        \"enable_hr\": False,\n",
    "        \"denoising_strength\": 0,\n",
    "        \"firstphase_width\": 0,\n",
    "        \"firstphase_height\": 0,\n",
    "        \"hr_scale\": 2,\n",
    "        \"hr_upscaler\": \"Hr Upscaler\",\n",
    "        \"hr_second_pass_steps\": 0,\n",
    "        \"hr_resize_x\": 0,\n",
    "        \"hr_resize_y\": 0,\n",
    "        \"styles\": [ ],\n",
    "        \"subseed\": -1,\n",
    "        \"subseed_strength\": 0,\n",
    "        \"seed_resize_from_h\": -1,\n",
    "        \"seed_resize_from_w\": -1,\n",
    "        \"sampler_name\": \"DPM++ SDE\",\n",
    "        \"batch_size\": 1,\n",
    "        \"n_iter\": 1,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"restore_faces\": False,\n",
    "        \"tiling\": False,\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"eta\": 0,\n",
    "        \"s_churn\": 0,\n",
    "        \"s_tmax\": 0,\n",
    "        \"s_tmin\": 0,\n",
    "        \"s_noise\": 1,\n",
    "        \"override_settings\": {\n",
    "            \"sd_model_checkpoint\": model_name\n",
    "        },\n",
    "        \"override_settings_restore_afterwards\": True,\n",
    "        \"script_args\": []\n",
    "    }  \n",
    "    response = requests.post(url=f'http://{webui_server_address}/sdapi/v1/txt2img', json=payload, headers = {\"Authorization\": webui_auth_header})\n",
    "    result = response.json()\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        f.write(base64.b64decode(result['images'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate an image, we will use the function `webui_generate_image`. Below are the parameters that this function accepts:\n",
    "- `model_name`: The name of the model to use for image generation (e.g., `juggernautXL_v9Rdphoto2Lightning`). This allows you to specify which model the WebUI should use for generating the image. The name used for this parameter is the same as the one displayed in the top left corner of the WebUI interface, without the file extension.\n",
    "- `prompt`: The main prompt used for the image generation, describing what the model should depict.\n",
    "- `negative_prompt`: A prompt that specifies what the model should avoid including in the image. This helps refine the output by filtering unwanted elements.\n",
    "- `seed`: The seed used for image generation. Using the same seed with the same prompts will consistently produce the same image. If you would like a random seed for each generation, set this value to -1.\n",
    "- `width`: The width (in pixels) of the generated image.\n",
    "- `height`: The height (in pixels) of the generated image.\n",
    "- `output_filename`: The file path where the generated image will be saved.\n",
    "\n",
    "Additionally, several parameters have been manually set internally in the function `webui_generate_image`, such as the number of steps (`steps`), the configuration scale (`cfg_scale`), and the sampler (`sampler_name`). While these are preconfigured for typical image generation, you are encouraged to explore and experiment with these parameters to see how they affect the quality and style of the generated images.\n",
    "\n",
    "Let's generate an image using the function `webui_generate_image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webui_generate_image(\"juggernautXL_v9Rdphoto2Lightning\", \"A serene lakeside at sunset, reflecting the vibrant colors of the sky\", \"\", -1, 1024, 1024, \"my_first_test_webui.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the image, we can visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"my_first_test_webui.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generating a Storyboard with WebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to generate individual images, let's explore how to create an image sequence within a storyboard context.\n",
    "\n",
    "A storyboard represents a sequence of events that together form a story. We can store this sequence of events in a list, where each item describes a scene in the story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_2 = [ \"Jake sprints through an abandoned street, gripping a bloodied baseball bat. His torn jacket flaps as he glances back at the horde of zombies shuffling behind him.\",\n",
    "            \"Jake barricades a door inside an empty diner, pushing tables and chairs against it. His bat rests beside him as he catches his breath, sweat dripping down his face.\",\n",
    "            \"Inside the dark diner, Jake peers out the window. His jacket is now tied around his waist, and he grips the bat tightly as zombies gather outside, their groans echoing in the distance.\",\n",
    "            \"Jake moves cautiously through the kitchen, picking up a flashlight. His bat is slung over his shoulder, and the flickering light casts shadows on the rusted metal walls.\",\n",
    "            \"Suddenly, a zombie lunges at Jake from the shadows. He swings his bat, smashing the creature aside. His face is determined, but fear flickers in his eyes.\",\n",
    "            \"Jake runs through the back alley, the bat still in hand. His jacket is gone now, and his shirt is ripped as he sprints toward a rusty truck parked at the end of the alley.\",\n",
    "            \"Jake jumps into the truck, slamming the door behind him. As the engine roars to life, he looks back at the zombies closing in, his grip still tight on the bat. He drives off into the night, the undead disappearing in the rearview mirror.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the sequence of events, we can call the function `webui_generate_image` for each story event to generate the corresponding scene illustrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\".//images\", exist_ok=True)  # create a subfolder to store the generated images\n",
    "\n",
    "for i in range(len(story_2)):\n",
    "    webui_generate_image(\"juggernautXL_v9Rdphoto2Lightning\", story_2[i], \"\", -1, 1024, 1024, \".//images//story_2_scene_\" + str(i + 1) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can display the generated images along with the corresponding text for each story event to create a storyboard representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(story_2)):\n",
    "    display(Image(filename=\".//images//story_2_scene_\" + str(i + 1) + \".png\", width=400))\n",
    "    print(story_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the generated storyboard looks visually appealing, it is clear that the main character, Jake, is represented inconsistently across the scenes. He appears with different physical attributes and completely different clothing, even in situations where she should be wearing the same outfit.\n",
    "\n",
    "**This brings us to your challenge: how can you improve character consistency in image sequences like the one illustrated in this notebook?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
